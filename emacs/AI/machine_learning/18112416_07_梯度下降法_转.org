* 20181124_07_梯度下降法_转

** 在Leetcode上刷算法题

** 梯度下降法
梯度下降法(Gradient Descent, GD)常用于求解无约束情况下凸函数(Convex Function)的极小值,是一种迭代类型的算法,因为凸函数只有一个极值点,故求解出来的极小值点就是函数的最小值点.
$$J(Q) = $$

梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向,因为该方向为当前位置的最快下降方向,所以梯度下降法也被称为"最速下降法".梯度下降法中越接近目标值,变量变化越小.计算公式如下:
$$\theta^{k + 1} = \theta^{k} - \alpha\nabla f(\alpha^{k})$$

 - $\alpha$被称为步长或者学习率(learning rate), 表示自变量$x$每次迭代变化的大小.
 - 收敛条件:当目标函数的函数值变化非常小的时候或者达到最大迭代次数的时候,就结束循环.

由于梯度下降法中负梯度方向作为变量的变化方向,所以有可能导致最终求解的值是局部最优解,所以在使用梯度下降的时候,一般需要进行一些高估策略:
 - 学习率的选择:学习率过大,表示每次迭代更新的时候变化比较大,有可能会路过最优解;学习率过小,表示每次迭代更新的时候变化比较小,就会导致迭代速度过慢,很长时间都不能结束;
 - 算法初始值的选择:初始值不同,最终获得的最小值也有可能不同,因为梯度下降法求解的是局部最优解,所以一般情况下,选择多次不同初始值运行算法,并最终返回损失函数最小情况下的结果值;
 - 标准化:由于样本不同特征的取值范围不同,可能会导致在各个不同参数上迭代速度不同,为了减少特征取值的影响,可以将特征进行标准化操作.

*批量梯度下降法(Batch Gradient Descent, BGD)*
使用所有样本在当前点的梯度值业对变量参数进行更新操作.
$$\theta^{k + 1} = \theta^{k} - \alpha\sum_{i=1}^m \nabla f_{\theta^{k}}(x^{i}))$$

*随机梯度下降法(Stochastic Gradient Descent, SGD)*
在变量参数的时候,选取一个样本的梯度值来更新参数.
$$ \theta^{k + 1}  = \theta^{k} - \alpha\nabla f_{\theta^{k}}(x^{i})$$

*小批量梯度下降法(Mini-batch Gradient Descent, MBGD)*
结合BGD和SGD的特性,从原始数据中,每次选择n个样本来更新参数值,一般n选择10.
$$ \theta^{k + 1} = \theta^{k} - \alpha\sum_{i=t}^{t+n-1} \nabla f_{\theta^{k}}(x^{i})$$

*** BGD, SGD, MBGD的区别
 - 当样本量为m的时候,每次迭代BGD算法中对于参数值更新一次,SGD算法中对于参数值更新m次,MBGD算法中对于参数值更新$m/n$次,相对来说SGD算法的速度最快
 - SGD算法中对于每个样本都需要更新参数值,当样本值不太正常的时候,就有可能会导致本次的参数更新会产生相反的影响,也就是说SGD算法的结果并不是完全收敛的,而是在收敛结果处波动的;
 - SGD算法是每个样本都更新一次参数值,所以SGD算法特别适合样本数据量大的情况以及在线机器学习(Online ML)

** Python科学计算库回顾
Python科学计算库主要是为机器学习提供了一些便捷,封装好的API,那么在实际工作中,主要是将其应用在机器学习的特征工程阶段,主要涉及到的库有以下几个:
 - NumPy:  数学计算基础库, N维数组,线性代数计算,傅立叶变换,随机数,图像处理,统计等
 - SciPy:  数值计算库,线性代数,拟合与优化,插值,数值积分,稀疏矩阵,图像处理,统计等(NumPy库依赖于SciPy库)
 - Pandas: 数据分析库,数据导入,整理,处理,分析等;
 - Matplotlib: 绘图库,绘制二维图形和图表

自己要学会看官网
