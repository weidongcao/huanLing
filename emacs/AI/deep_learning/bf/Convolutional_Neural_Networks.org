* 卷积神经网络
卷积神经网络(Convolutional Neural Networks, CNN),CNN可以有效降低反馈神经网络(传统神经网络)的复杂性,常见的CNN结构有LeNet-5, AlexNet, ZFNet, VGGNet, GoogleNet, ResNet等等,其中在LVSVRC2015冠军ResNet是AlexNet的20多倍,是VGGNet的8倍;从这些结构来讲CNN发展的一个方向就是层次的增加,通过这种方式可以利用增加的非线性得出目标函数的近似结构,同时得出更好的特征表达,但是这种方式导致了网络整体复杂性的增加,使网络更加难以优化,很容易过拟合.

CNN的应用主要是在图像分类和物品识别等应用场景应用比较多
** 卷积神经网络-主要层次
   - 数据输入层: Input Layer
   - 卷积计算层: CONV Layer
   - RelU激励层: RelU Layer
   - 池化层: Pooling Layer
   - 全连接层: FC Layer
** 卷积神经网络-Input Layer
   和神经网络/机器学习一样,对输入的数据进行预处理操作,需要进行预处理的主要原因是:
   - 输入数据单位不一样,可能会导致神经网络收敛速度慢,训练时间长,
   - 数据范围大的输入在模式分类中的作用可能偏大,而数据范围小的作用就有可能偏小
   - 由于神经网络中存在的激活函数是有值域限制的,因此需要将网络训练的目标数据映射到激活函数的值域
   - S型激活函数在(0, 1)以外区域很平缓,区分度太小.例如S型函数f(x),f(100)与f(5)只相差0.0067

** 人大脑识别图片过程
   人的大脑在识别图片的过程中,会由不同的皮质层处理不同方面的数据,比如:颜色,开头,光暗等,然后将不同皮质层的处理结果进行合并映射操作,得出最终的结果值,第一部分实质上是一个局部的观察结果,第二部分才是一个整体的结果合并.
   基于人脑的图片识别过程,我们可以认为图像的空间联系也是局部的像素联系比较紧密,而较远的像素相关性比较弱,所以每个神经元没有必要对全局图像进行感知,只要对局部进行感知,而在更高层次对局部的信息进行综合操作得出全局信息.

** 卷积神经网络正则化和Dropout
   神经网络的学习能力受神经元数目以及神经网络层次的影响,神经元数目越大,神经网络层次越高,那么神经网络的学习能力越强,那么就有可能出现过拟合的问题.
   Regularization:正则化,通过降低模型的复杂度,通过在Cost函数上添加一个正则项的方式来降低Overfitting,主要有L1和L2两种方式
   Dropout:通过随机删除神经网络中的神经元来解决Overfitting问题,在每次迭代的时候只使用部分神经元训练模型获取W和d的值,
   一般情况下,对于同一组训练数据,利用不同的神经网络训练之后,求其输出的平均值可以减少Overfitting.Dropout就是利用这个原理,每次丢掉一半左右的隐藏层神经元,相当于在不同的神经网络上进行训练,这样就减少了神经元之间的依赖性,即每个神经元不能依赖于某几个其它的神经元(指层与层之间相连接的神经元),使神经网络更加能学习到与其它神经元之间的更加健壮Robust(鲁棒性)的特征.另外Dropout不仅减少Overfitting,不能提高准确率
   正则化是通过给Cost函数添加正则项的方式来解决Overfitting,Dropout是通过直接修改神经网络的结构来解决Overfitting

** 局部响应归一化
   局部响应归一化层的建立是模仿生物神经系统的临近抵制机制,对局部神经元的活动创建竞争机制,使得局部响应比较大的值相对更大,这样更能凸显需要的特征,提高模型泛化能力.LRN一般是在激活,池化后进行的一中处理方法.用xx 表示通过激活函数后的输出值,对于第j个卷积核经过激活函数输出的神经元激活值xx,选取的n个卷积核卷积后的输出值,将它们在同一个位置上卷积后激活函数的输出值进行求和.其中(x, y)就代表特征层的坐标位置,i,j就代表不同的卷积核,N为总卷积核数目,k,a和\beta都是超参数,由验证数据集决定,一般取k=2,a=10-4,\beta = 0.75
   LRN:
   1. 增加了泛化能力,做了平滑处理
   2. LRN模仿了生物神经系统的侧抵制机制,提供局部神经元的竞争机制
      
a表示第i个核在位置(x,y)运用RelUe非线性化神经元输出,
n是同一位置上(x,y)临近的Kernel.

*如何调参*
SGD保持单一的学习率更新所有的权重
Adam通过梯度的一阶矩估计和二阶矩估计(偏方差),而为不同的参数设计对的自适应性学习率

结合2个梯度下降扩展式的优点:
1. AdaGrad适应性梯度算法:为每一个参数保留一个学习率以提升在稀疏梯度(计算机视觉和NLP)上的性能
2. RMSProp均方传播:基于权重梯度最近量级的均值为每一个参数适应性保留学习率

算法计算梯度的指移动均值,超参数\beta1和\beta2控制这些移动均值的衰减率.
1) Monmentum和RMSprop参数更新
   v_d_w = \beta1 * v_d_w + (1 - \beta1) * d_w    \beta1 一阶距估计的误差率
   v_d_b = \beta1 * v_d_b + (1 - \beta1) * d_b

   s_d_w = \beta2 * s_d_w + (1 - \beta2) * d_w   \beta2 二阶距估计的误差率
   s_d_b = \beta2 * s_d_b + (1 - \beta2) * d_b

2) 由于移动指数平均在迭代开始的初期会导致和开始的值有较大的差异,所以需要对d上岗帮偏差修正
   v_d_w = v_d_w / (1 - \beta1 ** t)    t代表第t轮迭代
   v_d_b_c = v_d_b / (1 - \beta1 ** t)

   s_d_w_c = s_d_w / (1 - \beta2 ** t)
   s_d_b_c = s_d_b / (1 - \beta2 ** t)

3) 对权重和偏置进行更新
   W = W - \alpha * (v_d_w_c / sqrt(s_d_w_c + epsilon)) epsilon 很小的值,防止
出现除以0的情况
   B = B - \alpha * (v_d_b_c / sqrt(s_d_b_c + epsilon)) \alpha 学习率
一般\alpha设置为0.001,\beta1 = 0.9, \beta2 = 0.999, epsilon = 10e-8

** TensorFlow自适应学习率
   tf.train.exponential_decay(learning_rate, globel_step, decay_setps, decay_rate_, staicaes, name)
   - learning_rate 学习率变量
   - globel_step   第几次迭代变量
   - decay_steps   常数,每多少次,进行一次参数更新
   - decay_rate    惩罚指数
   - staicaes      一般设置为False,表示不采用整除策略 

** 深度学习超参数之学习率调整策略
   fixed固定策略,学习率始终是一个固定值
   step均匀分步策略,如果设置为step,则还需要设置一个stepsize,返回
   base_lr * \gamma (floor(iter / stepsizez)) 其中iter表示当前的迭代次数. floor(9.9) = 9, 其功能是:向下取整.
   - base_lr * \gamma iter, iter为当前迭代次数
   - multistep 多分步或不均匀分步.刚开始训练时学习率一般设置较高,这样Loss和Accuracy下降很快,一般前200000次两者下降较快,后面可能就需要我们使用较小的学习率了.Step策略由于过于平均,而Loss和Accuracy的下降率在整个训练过程中又是一个平平均的过程,因此不是很合适.fixed手工调节起来又很麻烦,这是multistep可能就会派上用场了.multistep还需要设置一个stepvalue.这个参数和Step很相似,Step是均匀等间隔变化,而multistep则是根据Stepvalue值变化.


** AlexNet
   RPN本质是基于滑窗的无类别Object检测器

   RelU, Two-gpu提高训练速度

   重叠pool(overlapping pooling) 提高精度,不容易产生过拟合(第1, 2,5)

   LRN  提高精度(1, 2)之后

   Dropout 减少过拟合

** NIN模型
   Network-in-Network主要思想是:用全连接的多层感知机去代替传统的卷积过程,以获取特征更加全面的表达,同时,因为前面已经做了提升特征表达的过程,传统CNN最后的全连接层也被替换为一个全局平均池化层,因为作者认为此时的map已经具备分类足够的可信度了,它可以直接通过Softmax来计算Loss了.
